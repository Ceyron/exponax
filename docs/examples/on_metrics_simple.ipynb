{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The metrics of comparing fields in `Exponax`\n",
    "\n",
    "There are four major classes of metrics:\n",
    "\n",
    "1. Spatial-based (that work in physical space)\n",
    "2. Fourier-based (that work in the coefficient space)\n",
    "3. Correlation-based\n",
    "4. Derivative-based (which sugarcoat the functionalities to Fourier-based\n",
    "   approaches to achieve Sobolev-like norms)\n",
    "\n",
    "Class 1., 2., and 4. can be further divided into:\n",
    "1. Absolute metrics (i.e., related to the MAE)\n",
    "2. Absolute squared metrics (i.e., related to the MSE)\n",
    "3. Rooted metrics (i.e., related to the RMSE)\n",
    "\n",
    "Then for each of the three, there is both the absolute version and a\n",
    "relative/normalized version. For all spatial-based metrics, MAE, MSE, and RMSE\n",
    "also come with a symmetric version.\n",
    "\n",
    "All metrics computation work on single state arrays, i.e., arrays with a leading channel axis and one, two, or three subsequent spatial axes. **The arrays shall not have leading batch axes.** To work with batched arrays use `jax.vmap` and then reduce, e.g., by `jnp.mean`. Alternatively, use the convinience wrapper [`exponax.metrics.mean_metric`][].\n",
    "\n",
    "All metrics **sum over the channel axis**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import exponax as ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Reference and Predicted States\n",
    "\n",
    "Let's create two 1D fields using `DiffusedNoise`. We use the same random key\n",
    "so the signals share large-scale structure, but different `intensity` values\n",
    "produce different levels of smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_POINTS = 100\n",
    "\n",
    "u_ref = ex.ic.DiffusedNoise(1, intensity=0.001)(NUM_POINTS, key=jax.random.PRNGKey(0))\n",
    "u_pred = ex.ic.DiffusedNoise(1, intensity=0.0005)(NUM_POINTS, key=jax.random.PRNGKey(0))\n",
    "\n",
    "print(\"u_ref shape:\", u_ref.shape)\n",
    "print(\"u_pred shape:\", u_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3), sharey=True)\n",
    "ex.viz.plot_state_1d(u_ref, ax=axes[0])\n",
    "axes[0].set_title(\"Reference\")\n",
    "ex.viz.plot_state_1d(u_pred, ax=axes[1])\n",
    "axes[1].set_title(\"Prediction\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Standard Candidates: MAE, MSE, RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absolute Metrics\n",
    "\n",
    "The three workhorses of error measurement:\n",
    "\n",
    "- **MAE** (Mean Absolute Error) — related to the L1 norm of the error\n",
    "- **MSE** (Mean Squared Error) — related to the squared L2 norm of the error\n",
    "- **RMSE** (Root Mean Squared Error) — related to the L2 norm of the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = ex.metrics.MAE(u_pred, u_ref)\n",
    "mse = ex.metrics.MSE(u_pred, u_ref)\n",
    "rmse = ex.metrics.RMSE(u_pred, u_ref)\n",
    "\n",
    "print(f\"MAE  = {mae:.6f}\")\n",
    "print(f\"MSE  = {mse:.6f}\")\n",
    "print(f\"RMSE = {rmse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, `RMSE = sqrt(MSE)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"sqrt(MSE) = {jnp.sqrt(mse):.6f}\")\n",
    "print(f\"RMSE      = {rmse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If no reference is provided, the metric computes the norm of the state itself\n",
    "(i.e., the error against zero):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RMSE(u_ref, ref=None) = {ex.metrics.RMSE(u_ref):.6f}  (norm of u_ref)\")\n",
    "print(f\"RMSE(u_ref, ref=0)    = {ex.metrics.RMSE(u_ref, jnp.zeros_like(u_ref)):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized/Relative Metrics\n",
    "\n",
    "The normalized variants divide the absolute metric by the norm of the reference.\n",
    "This makes them **scale-invariant**: the error is expressed relative to the\n",
    "magnitude of the reference signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmae = ex.metrics.nMAE(u_pred, u_ref)\n",
    "nmse = ex.metrics.nMSE(u_pred, u_ref)\n",
    "nrmse = ex.metrics.nRMSE(u_pred, u_ref)\n",
    "\n",
    "print(f\"nMAE  = {nmae:.6f}\")\n",
    "print(f\"nMSE  = {nmse:.6f}\")\n",
    "print(f\"nRMSE = {nrmse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scale invariance:** multiplying both signals by the same constant leaves `nRMSE` unchanged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 10.0\n",
    "nrmse_original = ex.metrics.nRMSE(u_pred, u_ref)\n",
    "nrmse_scaled = ex.metrics.nRMSE(scale * u_pred, scale * u_ref)\n",
    "\n",
    "print(f\"nRMSE (original) = {nrmse_original:.6f}\")\n",
    "print(f\"nRMSE (x{scale:.0f})     = {nrmse_scaled:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also **symmetric** variants (`sMAE`, `sMSE`, `sRMSE`) that normalize\n",
    "by the sum of the norms of both signals. These are bounded between 0 and the\n",
    "number of channels C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"sRMSE = {ex.metrics.sRMSE(u_pred, u_ref):.6f}  (bounded in [0, C])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why it needs the domain size?\n",
    "\n",
    "All metrics approximate continuous integrals via the trapezoidal rule:\n",
    "\n",
    "$$\\text{MSE} \\approx \\frac{1}{L^D} \\int_\\Omega |u_{\\text{pred}} - u_{\\text{ref}}|^2 \\, dx \\approx \\frac{1}{N^D} \\sum_{i} |u_{\\text{pred},i} - u_{\\text{ref},i}|^2$$\n",
    "\n",
    "Because of the $(L/N)^D$ grid spacing factor, changing the `domain_extent`\n",
    "changes the absolute metric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for L in [1.0, 2.0, 5.0]:\n",
    "    mse_L = ex.metrics.MSE(u_pred, u_ref, domain_extent=L)\n",
    "    nrmse_L = ex.metrics.nRMSE(u_pred, u_ref, domain_extent=L)\n",
    "    print(f\"L={L:.1f}:  MSE={mse_L:.6f}   nRMSE={nrmse_L:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `MSE` changes with domain extent, but `nRMSE` stays constant because\n",
    "the domain-extent factor cancels in the ratio.\n",
    "\n",
    "**Practical advice:** When using metrics as optimization objectives (e.g., for\n",
    "training neural emulators), the domain extent is just a constant factor and does\n",
    "not affect the optimizer. But when comparing metrics across different setups,\n",
    "make sure `domain_extent` is set correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation\n",
    "\n",
    "Correlation measures the **shape similarity** between two fields via a\n",
    "normalized dot product. It ranges from -1 (anti-correlated) to +1 (identical\n",
    "shape). Crucially, it does *not* capture amplitude differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = ex.metrics.correlation(u_pred, u_ref)\n",
    "print(f\"correlation(u_pred, u_ref) = {corr:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identical fields -> correlation = 1.0\n",
    "print(f\"correlation(u_ref, u_ref)  = {ex.metrics.correlation(u_ref, u_ref):.6f}\")\n",
    "\n",
    "# Amplitude scaling does not change correlation\n",
    "print(\n",
    "    f\"correlation(10*u_pred, u_ref) = {ex.metrics.correlation(10 * u_pred, u_ref):.6f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two uncorrelated random fields -> correlation ~ 0\n",
    "u_random = ex.ic.DiffusedNoise(1, intensity=0.001)(\n",
    "    NUM_POINTS, key=jax.random.PRNGKey(42)\n",
    ")\n",
    "print(f\"correlation(u_ref, u_random) = {ex.metrics.correlation(u_ref, u_random):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourier-based Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait? Isn't that my MSE? A quick intro to Parseval's theorem\n",
    "\n",
    "**Parseval's theorem** tells us that the L2 norm in physical space equals the\n",
    "L2 norm in Fourier space. So `fourier_MSE` should numerically agree with `MSE`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_spatial = ex.metrics.MSE(u_pred, u_ref)\n",
    "mse_fourier = ex.metrics.fourier_MSE(u_pred, u_ref)\n",
    "\n",
    "print(f\"MSE (spatial)  = {mse_spatial:.8f}\")\n",
    "print(f\"fourier_MSE    = {mse_fourier:.8f}\")\n",
    "print(f\"Difference     = {abs(mse_spatial - mse_fourier):.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same holds for RMSE and nRMSE\n",
    "print(f\"RMSE         = {ex.metrics.RMSE(u_pred, u_ref):.8f}\")\n",
    "print(f\"fourier_RMSE = {ex.metrics.fourier_RMSE(u_pred, u_ref):.8f}\")\n",
    "print()\n",
    "print(f\"nRMSE         = {ex.metrics.nRMSE(u_pred, u_ref):.8f}\")\n",
    "print(f\"fourier_nRMSE = {ex.metrics.fourier_nRMSE(u_pred, u_ref):.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering and Scale-Specific Metrics\n",
    "\n",
    "The Fourier-based metrics accept `low` and `high` parameters to restrict the\n",
    "error computation to specific frequency (wavenumber) ranges. This lets you\n",
    "measure the error at different spatial scales.\n",
    "\n",
    "Let's demonstrate: we add high-frequency noise to a signal and show that the\n",
    "low-frequency error remains small while the full-spectrum error is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add high-frequency noise\n",
    "noise = 0.3 * ex.ic.DiffusedNoise(1, intensity=0.00001)(\n",
    "    NUM_POINTS, key=jax.random.PRNGKey(1)\n",
    ")\n",
    "u_noisy = u_ref + noise\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3), sharey=True)\n",
    "ex.viz.plot_state_1d(u_ref, ax=axes[0])\n",
    "axes[0].set_title(\"Reference\")\n",
    "ex.viz.plot_state_1d(u_noisy, ax=axes[1])\n",
    "axes[1].set_title(\"Reference + High-Freq Noise\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Full spectrum:\")\n",
    "print(f\"  fourier_nRMSE = {ex.metrics.fourier_nRMSE(u_noisy, u_ref):.6f}\")\n",
    "print()\n",
    "print(\"Low frequencies only (wavenumbers 0..5):\")\n",
    "print(f\"  fourier_nRMSE = {ex.metrics.fourier_nRMSE(u_noisy, u_ref, high=5):.6f}\")\n",
    "print()\n",
    "print(\"High frequencies only (wavenumbers 5..):\")\n",
    "print(f\"  fourier_nRMSE = {ex.metrics.fourier_nRMSE(u_noisy, u_ref, low=5):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also works in higher dimensions. Here is a 2D example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_ref_2d = ex.ic.DiffusedNoise(2, intensity=0.001)(64, key=jax.random.PRNGKey(0))\n",
    "u_pred_2d = ex.ic.DiffusedNoise(2, intensity=0.0005)(64, key=jax.random.PRNGKey(0))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 3.5))\n",
    "ex.viz.plot_state_2d(u_ref_2d, ax=axes[0])\n",
    "axes[0].set_title(\"Reference (2D)\")\n",
    "ex.viz.plot_state_2d(u_pred_2d, ax=axes[1])\n",
    "axes[1].set_title(\"Prediction (2D)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2D fourier_nRMSE (full spectrum):\")\n",
    "print(f\"  {ex.metrics.fourier_nRMSE(u_pred_2d, u_ref_2d):.6f}\")\n",
    "print(\"2D fourier_nRMSE (low freq, high=5):\")\n",
    "print(f\"  {ex.metrics.fourier_nRMSE(u_pred_2d, u_ref_2d, high=5):.6f}\")\n",
    "print(\"2D fourier_nRMSE (high freq, low=5):\")\n",
    "print(f\"  {ex.metrics.fourier_nRMSE(u_pred_2d, u_ref_2d, low=5):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics with derivatives\n",
    "\n",
    "The Fourier-based metrics support a `derivative_order` parameter. Setting\n",
    "`derivative_order=1` computes the error of the first derivative (done spectrally\n",
    "by multiplying with `ik` in Fourier space — no finite differences needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_0 = ex.metrics.fourier_RMSE(u_pred, u_ref)\n",
    "rmse_1 = ex.metrics.fourier_RMSE(u_pred, u_ref, derivative_order=1)\n",
    "rmse_2 = ex.metrics.fourier_RMSE(u_pred, u_ref, derivative_order=2)\n",
    "\n",
    "print(f\"fourier_RMSE (0th derivative) = {rmse_0:.6f}\")\n",
    "print(f\"fourier_RMSE (1st derivative) = {rmse_1:.6f}\")\n",
    "print(f\"fourier_RMSE (2nd derivative) = {rmse_2:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher derivative orders amplify high-frequency errors, making these metrics\n",
    "sensitive to fine-scale discrepancies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sobolev-like Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait? Who is Sobolev?\n",
    "\n",
    "The $H^1$ (Sobolev) norm combines the $L^2$ norm of the function itself with\n",
    "the $L^2$ norm of its gradient:\n",
    "\n",
    "$$\\|u\\|_{H^1}^2 = \\|u\\|_{L^2}^2 + \\|\\nabla u\\|_{L^2}^2$$\n",
    "\n",
    "This means $H^1$ metrics penalize both **value errors** and\n",
    "**derivative/smoothness errors**. They are especially useful for detecting\n",
    "predictions that have the right large-scale structure but wrong fine-scale\n",
    "details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_strongly_diffused = ex.ic.DiffusedNoise(1, intensity=0.001)(\n",
    "    100, key=jax.random.PRNGKey(0)\n",
    ")\n",
    "u_less_diffused = ex.ic.DiffusedNoise(1, intensity=0.0003)(\n",
    "    100, key=jax.random.PRNGKey(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex.viz.plot_state_1d(jnp.concatenate([u_strongly_diffused, u_less_diffused]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrmse_val = ex.metrics.nRMSE(u_strongly_diffused, u_less_diffused)\n",
    "h1_nrmse_val = ex.metrics.H1_nRMSE(u_strongly_diffused, u_less_diffused)\n",
    "\n",
    "print(f\"nRMSE    = {nrmse_val:.6f}\")\n",
    "print(f\"H1_nRMSE = {h1_nrmse_val:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `H1_nRMSE` is significantly larger than `nRMSE` because the two fields\n",
    "differ substantially in their gradient (smoothness) content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application: Detecting Blurry Predictions of Neural Emulators\n",
    "\n",
    "A common failure mode of neural PDE emulators is producing **blurry**\n",
    "predictions: the large-scale structure is correct, but fine details are smeared\n",
    "out. Standard `nRMSE` may look acceptable, but `H1_nRMSE` reveals the\n",
    "smoothness mismatch.\n",
    "\n",
    "Let's simulate this scenario with the 1D Burgers equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a Burgers simulation to get a \"ground truth\" state\n",
    "burgers = ex.stepper.Burgers(\n",
    "    num_spatial_dims=1,\n",
    "    domain_extent=1.0,\n",
    "    num_points=100,\n",
    "    dt=0.1,\n",
    "    diffusivity=0.01,\n",
    ")\n",
    "\n",
    "u0 = ex.ic.DiffusedNoise(1, intensity=0.0005, max_one=True)(\n",
    "    100, key=jax.random.PRNGKey(0)\n",
    ")\n",
    "# Step forward a few times to develop sharp gradients\n",
    "trj = ex.rollout(burgers, 5, include_init=True)(u0)\n",
    "u_truth = trj[-1]  # ground truth at t=0.5\n",
    "\n",
    "print(\"u_truth shape:\", u_truth.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a \"blurry\" prediction by running with much higher diffusivity\n",
    "burgers_blurry = ex.stepper.Burgers(\n",
    "    num_spatial_dims=1,\n",
    "    domain_extent=1.0,\n",
    "    num_points=100,\n",
    "    dt=0.1,\n",
    "    diffusivity=0.05,  # 5x more diffusion\n",
    ")\n",
    "trj_blurry = ex.rollout(burgers_blurry, 5, include_init=True)(u0)\n",
    "u_blurry = trj_blurry[-1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "ex.viz.plot_state_1d(\n",
    "    jnp.concatenate([u_truth, u_blurry]),\n",
    "    ax=ax,\n",
    "    labels=[\"Ground Truth\", \"Blurry Prediction\"],\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_title(\"Burgers @ t=0.5\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"nRMSE    = {ex.metrics.nRMSE(u_blurry, u_truth):.6f}\")\n",
    "print(f\"H1_nRMSE = {ex.metrics.H1_nRMSE(u_blurry, u_truth):.6f}\")\n",
    "print()\n",
    "print(\"The H1 metric is more sensitive to the blurriness because it also\")\n",
    "print(\"penalizes the smoothed-out gradients.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Batches\n",
    "\n",
    "All metrics operate on single states (no batch axis). To compute metrics over a\n",
    "batch, use `exponax.metrics.mean_metric` which vmaps and averages for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a small batch of initial conditions\n",
    "keys = jax.random.split(jax.random.PRNGKey(0), 8)\n",
    "ic_gen_ref = ex.ic.DiffusedNoise(1, intensity=0.001)\n",
    "ic_gen_pred = ex.ic.DiffusedNoise(1, intensity=0.0005)\n",
    "\n",
    "batch_ref = jax.vmap(lambda k: ic_gen_ref(NUM_POINTS, key=k))(keys)\n",
    "batch_pred = jax.vmap(lambda k: ic_gen_pred(NUM_POINTS, key=k))(keys)\n",
    "\n",
    "print(f\"batch_ref shape:  {batch_ref.shape}\")\n",
    "print(f\"batch_pred shape: {batch_pred.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_nrmse = ex.metrics.mean_metric(ex.metrics.nRMSE, batch_pred, batch_ref)\n",
    "mean_h1 = ex.metrics.mean_metric(ex.metrics.H1_nRMSE, batch_pred, batch_ref)\n",
    "\n",
    "print(f\"Mean nRMSE    over batch = {mean_nrmse:.6f}\")\n",
    "print(f\"Mean H1_nRMSE over batch = {mean_h1:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exponax_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
